+++
title = "Publications"
draft = false
menu  = "main"
weight = 1
+++

Below are some recent publications:

* [**Probing the robustness of large language models safety to latent Perturbations**](https://arxiv.org/pdf/2506.16078)  
Arxiv 2025 (under review)
* [**SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs**](https://arxiv.org/html/2509.26100v1)  
Arxiv 2025 (under review)
* [**GhostEI-Bench: Do Mobile Agent Withstand Environmental Injection in Dynamic On-Device Environments?**]
Arxiv 2025 (under review)
* [**A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports**](https://arxiv.org/pdf/2510.02190)  
Arxiv 2025 (under review)
* [**FreezeVLA: Action-Freezing Attacks on Vision-Language-Action Models**](https://arxiv.org/pdf/2509.19870)  
Arxiv 2025 (under review)
* [**FA2RM: Adversarial-Augmented Reward Model**]
Arxiv 2025 (under review)
* [**Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?**](https://arxiv.org/abs/2506.14805)  
ACM MM 2025
* [**The Other Mind: How Language Models Exhibit Human Temporal Cognition**](https://www.arxiv.org/abs/2507.15851v1)  
AAAI 2026
* [**SafeVid: Toward Safety Aligned Video Large Multimodal Models**](https://arxiv.org/abs/2505.11926)
NeurIPS 2025
* [**JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models**](https://arxiv.org/abs/2505.19610)  
NeurIPS 2025
* [**LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models**](https://arxiv.org/html/2508.12733v1)  
NeurIPS 2025
* [**Reflection-Bench: Evaluating Epistemic Agency in Large Language Models**](https://arxiv.org/pdf/2410.16270)  
ICML 2025
* [**A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos**](https://arxiv.org/abs/2502.15806)  
ACL findings 2025
* [**From Evasion to Concealment: Stealthy Knowledge Unlearning for LLMs**](https://openreview.net/forum?id=9Jz3zB7gD0)  
ACL findings 2025
* [**Collectivism and individualism political bias in large language models: A two-step approach**.](https://journals.sagepub.com/doi/10.1177/20539517251343861)  
Big Data & Society 2025
* [**HoneypotNet: Backdoor Attacks Against Model Extraction**](https://arxiv.org/pdf/2501.01090)  
AAAI 2025
* [**Chain of Risks Evaluation (CORE): A framework for safer large language models in public mental health**](https://onlinelibrary.wiley.com/doi/10.1111/pcn.13781)  
Psychiatry and Clinical Neurosciences 2025
* [**IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves**](https://arxiv.org/abs/2411.00827)  
ICCV 2025
* **StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data**  
ICCV 2025
* [**MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models**](https://arxiv.org/abs/2406.07594)  
NeurIPS 2024
* [**ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models**](https://arxiv.org/abs/2406.14952)  
EMNLP 2024
* [**Fake Alignment: Are LLMs Really Aligned Well?**](https://arxiv.org/abs/2311.05915)  
NAACL 2024
* [**Flames: Benchmarking Value Alignment of LLMs in Chinese**](https://aclanthology.org/2024.naacl-long.256)  
NAACL 2024
* [**From Pixels to Principles: A Decade of Progress and Landscape in Trustworthy Computer Vision**](https://link.springer.com/article/10.1007/s11948-024-00480-6)  
Science and Engineering Ethics 2024
* [**From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities**](https://arxiv.org/abs/2401.15071)  
Report 2024