baseURL = "https://TENGYan666.github.io/"
languageCode = "en-us"
title = "TENG Yan's Homepage"
theme = "hugo-blog-awesome"

[[menu.main]]
  name   = "Publications"
  url    = "/publications/"
  weight = 1

[Languages.en-us.params.author]
  avatar      = "avater.jpg"
  intro       = "TENG Yan"
  description    = "Shanghai Artificiall Intelligence Laboratory"  # 可以留空或删掉
  extraParagraphs = [
    "TENG Yan is currently a research scientist of Safe & Trustworhty Center at Shanghai AI Lab. She received her PhD from Delft University of Technology. Her research focuses on AI safety, value-driven design and alignment of AI, AI governance, and interdisciplinary AI research.",
    " ",
    "We have opening positions for **full-time employees, interns, joint PhDs**."
  ]
# ===================== Recent Research =====================

# -------- AI SAFETY AND ALIGNMENT --------
[[params.recentResearch]]
  topic = "AI Safety and Alignment"
  title = "SafeVid: Toward Safety Aligned Video Large Multimodal Models"
  venue = "NeurIPS"
  year  = 2025
  url   = "https://arxiv.org/abs/2505.11926"

[[params.recentResearch]]
  topic = "AI Safety and Alignment"
  title = "A2RM: Adversarial-Augmented Reward Model"
  venue = "Submitted to ICLR"
  year  = 2026

[[params.recentResearch]]
  topic = "AI Safety and Alignment"
  title = "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45∘ Law"
  venue = "arxiv"
  year  = 2026
  url   = "https://arxiv.org/abs/2507.18576"

# -------- ADVERSARIAL ATTACK AND DEFENSE --------
[[params.recentResearch]]
  topic = "Adversarial Attack and Defense"
  title = "Probing the robustness of large language models safety to latent Perturbations"
  venue = "Submitted to ICLR"
  year  = 2026
  url   = "https://arxiv.org/pdf/2506.16078"

[[params.recentResearch]]
  topic = "Adversarial Attack and Defense"
  title = "FreezeVLA: Action-Freezing Attacks on Vision-Language-Action Models"
  venue = "Submitted to ICLR"
  year  = 2026
  url   = "https://arxiv.org/pdf/2509.19870"

[[params.recentResearch]]
  topic = "Adversarial Attack and Defense"
  title = "JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models"
  venue = "NeurIPS"
  year  = 2025
  url   = "https://arxiv.org/abs/2505.19610"

[[params.recentResearch]]
  topic = "Adversarial Attack and Defense"
  title = "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos"
  venue = "ACL findings"
  year  = 2025
  url   = "https://arxiv.org/abs/2502.15806"

[[params.recentResearch]]
  topic = "Adversarial Attack and Defense"
  title = "From Evasion to Concealment: Stealthy Knowledge Unlearning for LLMs"
  venue = "ACL findings"
  year  = 2025
  url   = "https://arxiv.org/pdf/2410.16270"

[[params.recentResearch]]
  topic = "Adversarial Attack and Defense"
  title = "HoneypotNet: Backdoor Attacks Against Model Extraction"
  venue = "AAAI"
  year  = 2025
  url   = "https://arxiv.org/pdf/2501.01090"

[[params.recentResearch]]
  topic = "Adversarial Attack and Defense"
  title = "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data"
  venue = "ICCV"
  year  = 2025

[[params.recentResearch]]
  topic = "Adversarial Attack and Defense"
  title = "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves"
  venue = "ICCV"
  year  = 2025
  url   = "https://arxiv.org/abs/2411.00827"

# -------- EVALUATION AND BENCHMARK --------
[[params.recentResearch]]
  topic = "Evaluation and Benchmark"
  title = "SafeEvalAgent: Toward Agentic and Self -Evolving Safety Evaluation of LLMs"
  venue = "Submitted to ICLR"
  year  = 2026
  url   = "https://arxiv.org/html/2509.2100v1"

[[params.recentResearch]]
  topic = "Evaluation and Benchmark"
  title = "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports"
  venue = "Submitted to ICLR"
  year  = 2026
  url   = "https://arxiv.org/pdf/2510.02190"

[[params.recentResearch]]
  topic = "Evaluation and Benchmark"
  title = "GhostEI-Bench: Do Mobile Agent Withstand Environmental Injection in Dynamic On-Device Environments?"
  venue = "Submitted to ICLR"
  year  = 2026

[[params.recentResearch]]
  topic = "Evaluation and Benchmark"
  title = "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?"
  venue = "ACM MM"
  year  = 2025
  url   = "https://arxiv.org/abs/2506.14805"

[[params.recentResearch]]
  topic = "Evaluation and Benchmark"
  title = "LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models"
  venue = "Arxiv"
  year  = 2025
  url   = "https://arxiv.org/html/2508.12733v1"

[[params.recentResearch]]
  topic = "Evaluation and Benchmark"
  title = "Reflection-Bench: Evaluating Epistemic Agency in Large Language Models"
  venue = "ICML"
  year  = 2025
  url   = "https://arxiv.org/pdf/2410.16270"

# -------- INTERDISCIPLINARY RESEARCH --------
[[params.recentResearch]]
  topic = "Interdisciplinary Research"
  title = "The Other Mind: How Language Models Exhibit Human Temporal Cognition"
  venue = "AAAI"
  year  = 2026
  url   = "https://www.arxiv.org/abs/2507.15851v1"

[[params.recentResearch]]
  topic = "Interdisciplinary Research"
  title = "Collectivism and individualism political bias in large language models: A two-step approach"
  venue = "Big Data & Society"
  year  = 2025
  url   = "https://journals.sagepub.com/doi/10.1177/20539517251343861"

[[params.recentResearch]]
  topic = "Interdisciplinary Research"
  title = "Chain of Risks Evaluation (CORE): A framework for safer large language models in public mental health"
  venue = "Psychiatry and Clinical Neurosciences"
  year  = 2025
  url   = "https://onlinelibrary.wiley.com/doi/10.1111/pcn.13781"


# contact
[[params.socialIcons]]
name = "github"
url = "https://github.com/AI45Lab"

[[params.socialIcons]]
name = "email"
url = "tengyan@pjlab.org.cn"