<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=content-type content="text/html"><meta name=viewport content="width=device-width,initial-scale=1"><title itemprop=name>Publications | TENG Yan's Homepage</title><meta property="og:title" content="Publications | TENG Yan's Homepage"><meta name=twitter:title content="Publications | TENG Yan's Homepage"><meta itemprop=name content="Publications | TENG Yan's Homepage"><meta name=application-name content="Publications | TENG Yan's Homepage"><meta property="og:site_name" content><meta name=description content><meta itemprop=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:locale" content="en-us"><meta name=language content="en-us"><link rel=alternate hreflang=en-us href=https://TENGYan666.github.io/publications/ title><meta name=generator content="Hugo 0.152.2"><meta property="og:url" content="https://TENGYan666.github.io/publications/"><meta property="og:site_name" content="TENG Yan's Homepage"><meta property="og:title" content="Publications"><meta property="og:description" content="Below are some recent publications:
Probing the robustness of large language models safety to latent Perturbations
Arxiv 2025 (under review) SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs
Arxiv 2025 (under review) [GhostEI-Bench: Do Mobile Agent Withstand Environmental Injection in Dynamic On-Device Environments?] Arxiv 2025 (under review) A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports
Arxiv 2025 (under review) FreezeVLA: Action-Freezing Attacks on Vision-Language-Action Models
Arxiv 2025 (under review) [FA2RM: Adversarial-Augmented Reward Model] Arxiv 2025 (under review) Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?
ACM MM 2025 The Other Mind: How Language Models Exhibit Human Temporal Cognition
AAAI 2026 SafeVid: Toward Safety Aligned Video Large Multimodal Models NeurIPS 2025 JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models
NeurIPS 2025 LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models
NeurIPS 2025 Reflection-Bench: Evaluating Epistemic Agency in Large Language Models
ICML 2025 A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos
ACL findings 2025 From Evasion to Concealment: Stealthy Knowledge Unlearning for LLMs
ACL findings 2025 Collectivism and individualism political bias in large language models: A two-step approach.
Big Data & Society 2025 HoneypotNet: Backdoor Attacks Against Model Extraction
AAAI 2025 Chain of Risks Evaluation (CORE): A framework for safer large language models in public mental health
Psychiatry and Clinical Neurosciences 2025 IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves
ICCV 2025 StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data
ICCV 2025 MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models
NeurIPS 2024 ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models
EMNLP 2024 Fake Alignment: Are LLMs Really Aligned Well?
NAACL 2024 Flames: Benchmarking Value Alignment of LLMs in Chinese
NAACL 2024 From Pixels to Principles: A Decade of Progress and Landscape in Trustworthy Computer Vision
Science and Engineering Ethics 2024 From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities
Report 2024"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta name=twitter:card content="summary"><meta name=twitter:title content="Publications"><meta name=twitter:description content="Below are some recent publications:
Probing the robustness of large language models safety to latent Perturbations
Arxiv 2025 (under review) SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs
Arxiv 2025 (under review) [GhostEI-Bench: Do Mobile Agent Withstand Environmental Injection in Dynamic On-Device Environments?] Arxiv 2025 (under review) A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports
Arxiv 2025 (under review) FreezeVLA: Action-Freezing Attacks on Vision-Language-Action Models
Arxiv 2025 (under review) [FA2RM: Adversarial-Augmented Reward Model] Arxiv 2025 (under review) Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?
ACM MM 2025 The Other Mind: How Language Models Exhibit Human Temporal Cognition
AAAI 2026 SafeVid: Toward Safety Aligned Video Large Multimodal Models NeurIPS 2025 JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models
NeurIPS 2025 LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models
NeurIPS 2025 Reflection-Bench: Evaluating Epistemic Agency in Large Language Models
ICML 2025 A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos
ACL findings 2025 From Evasion to Concealment: Stealthy Knowledge Unlearning for LLMs
ACL findings 2025 Collectivism and individualism political bias in large language models: A two-step approach.
Big Data & Society 2025 HoneypotNet: Backdoor Attacks Against Model Extraction
AAAI 2025 Chain of Risks Evaluation (CORE): A framework for safer large language models in public mental health
Psychiatry and Clinical Neurosciences 2025 IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves
ICCV 2025 StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data
ICCV 2025 MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models
NeurIPS 2024 ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models
EMNLP 2024 Fake Alignment: Are LLMs Really Aligned Well?
NAACL 2024 Flames: Benchmarking Value Alignment of LLMs in Chinese
NAACL 2024 From Pixels to Principles: A Decade of Progress and Landscape in Trustworthy Computer Vision
Science and Engineering Ethics 2024 From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities
Report 2024"><link rel=canonical href=https://TENGYan666.github.io/publications/><link href=/style.min.e390ba7da26222f4dc42a349955d76dbbe44e5ce2535a43de5a70633a0a9ec3c.css rel=stylesheet><link href=/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css rel=stylesheet><link rel=icon href=/avater_circle.png type=image/jpeg><link rel=manifest href=https://TENGYan666.github.io/site.webmanifest><meta name=msapplication-config content="/browserconfig.xml"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#434648"></head><body data-theme class=notransition><script src=/js/theme.min.8961c317c5b88b953fe27525839672c9343f1058ab044696ca225656c8ba2ab0.js integrity="sha256-iWHDF8W4i5U/4nUlg5ZyyTQ/EFirBEaWyiJWVsi6KrA="></script><div class=navbar role=navigation><nav class=menu aria-label="Main Navigation"><a href=https://TENGYan666.github.io/ class=logo><svg width="25" height="25" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-home"><title>Home</title><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
</a><input type=checkbox id=menu-trigger class=menu-trigger>
<label for=menu-trigger><span class=menu-icon><svg width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7H3.40726"/><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488H3.49301"/><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"/><path stroke-linecap="round" stroke-linejoin="round" d="M.5 12.5V1.5c0-.552285.447715-1 1-1h11C13.0523.5 13.5.947715 13.5 1.5v11C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C.947715 13.5.5 13.0523.5 12.5z"/></svg></span></label><div class=trigger><ul class=trigger-container><li><a class="menu-link active" href=/publications/>Publications</a></li><li class=menu-separator><span>|</span></li></ul><a id=mode href=#><svg class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1"><title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1=".5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1=".5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/></g></svg>
<svg class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1"><title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1=".5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1=".5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/></g></svg></a></div></nav></div><div class="wrapper post"><main class=page-content aria-label=Content><article><header class=header><h1 class=header-title>Publications</h1></header><div class=page-content><p>Below are some recent publications:</p><ul><li><a href=https://arxiv.org/pdf/2506.16078><strong>Probing the robustness of large language models safety to latent Perturbations</strong></a><br>Arxiv 2025 (under review)</li><li><a href=https://arxiv.org/html/2509.26100v1><strong>SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs</strong></a><br>Arxiv 2025 (under review)</li><li>[<strong>GhostEI-Bench: Do Mobile Agent Withstand Environmental Injection in Dynamic On-Device Environments?</strong>]
Arxiv 2025 (under review)</li><li><a href=https://arxiv.org/pdf/2510.02190><strong>A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports</strong></a><br>Arxiv 2025 (under review)</li><li><a href=https://arxiv.org/pdf/2509.19870><strong>FreezeVLA: Action-Freezing Attacks on Vision-Language-Action Models</strong></a><br>Arxiv 2025 (under review)</li><li>[<strong>FA2RM: Adversarial-Augmented Reward Model</strong>]
Arxiv 2025 (under review)</li><li><a href=https://arxiv.org/abs/2506.14805><strong>Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?</strong></a><br>ACM MM 2025</li><li><a href=https://www.arxiv.org/abs/2507.15851v1><strong>The Other Mind: How Language Models Exhibit Human Temporal Cognition</strong></a><br>AAAI 2026</li><li><a href=https://arxiv.org/abs/2505.11926><strong>SafeVid: Toward Safety Aligned Video Large Multimodal Models</strong></a>
NeurIPS 2025</li><li><a href=https://arxiv.org/abs/2505.19610><strong>JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models</strong></a><br>NeurIPS 2025</li><li><a href=https://arxiv.org/html/2508.12733v1><strong>LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models</strong></a><br>NeurIPS 2025</li><li><a href=https://arxiv.org/pdf/2410.16270><strong>Reflection-Bench: Evaluating Epistemic Agency in Large Language Models</strong></a><br>ICML 2025</li><li><a href=https://arxiv.org/abs/2502.15806><strong>A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos</strong></a><br>ACL findings 2025</li><li><a href="https://openreview.net/forum?id=9Jz3zB7gD0"><strong>From Evasion to Concealment: Stealthy Knowledge Unlearning for LLMs</strong></a><br>ACL findings 2025</li><li><a href=https://journals.sagepub.com/doi/10.1177/20539517251343861><strong>Collectivism and individualism political bias in large language models: A two-step approach</strong>.</a><br>Big Data & Society 2025</li><li><a href=https://arxiv.org/pdf/2501.01090><strong>HoneypotNet: Backdoor Attacks Against Model Extraction</strong></a><br>AAAI 2025</li><li><a href=https://onlinelibrary.wiley.com/doi/10.1111/pcn.13781><strong>Chain of Risks Evaluation (CORE): A framework for safer large language models in public mental health</strong></a><br>Psychiatry and Clinical Neurosciences 2025</li><li><a href=https://arxiv.org/abs/2411.00827><strong>IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves</strong></a><br>ICCV 2025</li><li><strong>StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data</strong><br>ICCV 2025</li><li><a href=https://arxiv.org/abs/2406.07594><strong>MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models</strong></a><br>NeurIPS 2024</li><li><a href=https://arxiv.org/abs/2406.14952><strong>ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models</strong></a><br>EMNLP 2024</li><li><a href=https://arxiv.org/abs/2311.05915><strong>Fake Alignment: Are LLMs Really Aligned Well?</strong></a><br>NAACL 2024</li><li><a href=https://aclanthology.org/2024.naacl-long.256><strong>Flames: Benchmarking Value Alignment of LLMs in Chinese</strong></a><br>NAACL 2024</li><li><a href=https://link.springer.com/article/10.1007/s11948-024-00480-6><strong>From Pixels to Principles: A Decade of Progress and Landscape in Trustworthy Computer Vision</strong></a><br>Science and Engineering Ethics 2024</li><li><a href=https://arxiv.org/abs/2401.15071><strong>From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities</strong></a><br>Report 2024</li></ul></div></article></main></div><footer class=footer><span class=footer_item></span>&nbsp;<div class=footer_social-icons><a href=https://github.com/AI45Lab target=_blank rel="noopener noreferrer me" title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=tengyan@pjlab.org.cn target=_blank rel="noopener noreferrer me" title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><small class=footer_copyright>Â© 2025 .
Powered by <a href=https://github.com/hugo-sid/hugo-blog-awesome target=_blank rel=noopener>Hugo blog awesome</a>.</small></footer><script src=https://TENGYan666.github.io/js/main.min.4ee188e1744c19816e95a540b2650ed9f033ea0371e74eac8e717355cfca8741.js integrity="sha256-TuGI4XRMGYFulaVAsmUO2fAz6gNx506sjnFzVc/Kh0E="></script></body></html>